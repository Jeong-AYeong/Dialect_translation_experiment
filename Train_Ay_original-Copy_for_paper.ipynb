{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M46doxPBWMH"
   },
   "source": [
    "#### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 21814,
     "status": "ok",
     "timestamp": 1717558921073,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "7W3mKuyj_uSx",
    "outputId": "eaccba74-f8f0-4c83-c6b7-ba162cfb3329"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>난 진짜 벌초 할때 뱀나오잖아 뱀도</td>\n",
       "      <td>난 진짜 벌초 할때 뱀나오잖아 뱀도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어어 그니까</td>\n",
       "      <td>어어 그니까</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 잘도</td>\n",
       "      <td>아 잘도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>예초기 뱀도 짤려</td>\n",
       "      <td>예초기 뱀도 짤려</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>어 그니까 완전 끔찍해</td>\n",
       "      <td>어 그니까 완전 끔찍해</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         standard_form         dialect_form\n",
       "0  난 진짜 벌초 할때 뱀나오잖아 뱀도  난 진짜 벌초 할때 뱀나오잖아 뱀도\n",
       "1               어어 그니까               어어 그니까\n",
       "2                 아 잘도                 아 잘도\n",
       "3            예초기 뱀도 짤려            예초기 뱀도 짤려\n",
       "4         어 그니까 완전 끔찍해         어 그니까 완전 끔찍해"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd # Gzip 압축된 CSV 파일 불러오기\n",
    "\n",
    "df = pd.read_csv(\"data.csv.gz\", compression='gzip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1717558344705,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "4hB_tLVoBEZq",
    "outputId": "60027332-e65d-4c3b-f360-a060f30179a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2774257 entries, 0 to 2774256\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   standard_form  object\n",
      " 1   dialect_form   object\n",
      "dtypes: object(2)\n",
      "memory usage: 42.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3457,
     "status": "ok",
     "timestamp": 1717558927221,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "5KNQZpw9AIld",
    "outputId": "c2539a06-e39b-40b0-d233-10b0e440485c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1116389 entries, 6 to 2774254\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   standard_form  1116389 non-null  object\n",
      " 1   dialect_form   1116389 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 25.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거\n",
    "df_filtered = df[df[\"standard_form\"] != df[\"dialect_form\"]]\n",
    "df_filtered = df_filtered.drop_duplicates()\n",
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1717558930657,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "dz1DbFSTBIIz",
    "outputId": "0cf9adf3-106c-4ffe-da10-de414b7ac828"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>예전에 그렇게</td>\n",
       "      <td>예전에 겅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아 그러니까 예초 할땐 조심해야 돼</td>\n",
       "      <td>아 겅하니까이 예초 할땐 조심해야 돼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러고 예초 벌초(하다그네)/(하다가)</td>\n",
       "      <td>겅허고 예초 벌초(하다그네)/(하다가)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어 그거 튀겨서</td>\n",
       "      <td>어 그거 튀겨그네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>돌멩이 해 가지고 실명된사람 있어</td>\n",
       "      <td>돌멩이 해 가지고 실명된사람 이서</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           standard_form           dialect_form\n",
       "0                예전에 그렇게                  예전에 겅\n",
       "1    아 그러니까 예초 할땐 조심해야 돼   아 겅하니까이 예초 할땐 조심해야 돼\n",
       "2  그러고 예초 벌초(하다그네)/(하다가)  겅허고 예초 벌초(하다그네)/(하다가)\n",
       "3               어 그거 튀겨서              어 그거 튀겨그네\n",
       "4     돌멩이 해 가지고 실명된사람 있어     돌멩이 해 가지고 실명된사람 이서"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>막 그냥 예 습기가</td>\n",
       "      <td>막 그냥 예 습기가</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그니까 제주도 사람 그런 이</td>\n",
       "      <td>그니까 제주도 사람 그런 이</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그런데 이 지금은 비 오는 날 별로 안 좋아</td>\n",
       "      <td>그런데 이 지금은 비 오는 날 별로 안 좋아</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그리고 안 되면은 이 그만둘 생각도 해야 돼</td>\n",
       "      <td>그리고 안 되면은 이 그만둘 생각도 해야 돼</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>왜냐면 이 난 겨울에 롱패딩 생활밖에 안 해</td>\n",
       "      <td>왜냐면 이 난 겨울에 롱패딩 생활밖에 안 해</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              standard_form              dialect_form  similarity\n",
       "0                막 그냥 예 습기가                막 그냥 예 습기가         1.0\n",
       "1           그니까 제주도 사람 그런 이           그니까 제주도 사람 그런 이         1.0\n",
       "2  그런데 이 지금은 비 오는 날 별로 안 좋아  그런데 이 지금은 비 오는 날 별로 안 좋아         1.0\n",
       "3  그리고 안 되면은 이 그만둘 생각도 해야 돼  그리고 안 되면은 이 그만둘 생각도 해야 돼         1.0\n",
       "4  왜냐면 이 난 겨울에 롱패딩 생활밖에 안 해  왜냐면 이 난 겨울에 롱패딩 생활밖에 안 해         1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd # Gzip 압축된 CSV 파일 불러오기\n",
    "\n",
    "df_filtered = pd.read_csv(\"high_similarity_data.csv\")\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>전봇대 잡아서 막 우는 게 비디오에 거기 나온 거야</td>\n",
       "      <td>전봇대 잡앙 막 우는 게 비디오에 거기 나온 거라</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>꽃집에 갔다 왔어요 뭐하러 어두운 시간에 갔다왔습니까</td>\n",
       "      <td>꽃집에 갔다 왔수과 뭐하랜 어두운 시간에 갔다완마씨</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>진짜 훠넌 거 진짜 통으로 사다가 먹으니까 따준다고했지</td>\n",
       "      <td>진짜 훠넌 거 진짜 통으로 사단 먹으난 따줄켄햄찌</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아 그렇구나 그러면 내가 말할게 내가 아는 건 뭐냐 하면</td>\n",
       "      <td>아 기구나 겨믄 내가 골아볼게 내가 아는 건 뭔고 하면</td>\n",
       "      <td>0.899999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그게 복이 되는줄을 아직도 몰라요</td>\n",
       "      <td>그게 복이 되는줄을 아직도 모름수과</td>\n",
       "      <td>0.899999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     standard_form                    dialect_form  similarity\n",
       "0     전봇대 잡아서 막 우는 게 비디오에 거기 나온 거야     전봇대 잡앙 막 우는 게 비디오에 거기 나온 거라    0.900000\n",
       "1    꽃집에 갔다 왔어요 뭐하러 어두운 시간에 갔다왔습니까    꽃집에 갔다 왔수과 뭐하랜 어두운 시간에 갔다완마씨    0.900000\n",
       "2   진짜 훠넌 거 진짜 통으로 사다가 먹으니까 따준다고했지     진짜 훠넌 거 진짜 통으로 사단 먹으난 따줄켄햄찌    0.900000\n",
       "3  아 그렇구나 그러면 내가 말할게 내가 아는 건 뭐냐 하면  아 기구나 겨믄 내가 골아볼게 내가 아는 건 뭔고 하면    0.899999\n",
       "4               그게 복이 되는줄을 아직도 몰라요             그게 복이 되는줄을 아직도 모름수과    0.899999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_low = pd.read_csv(\"low_similarity_data.csv\")\n",
    "df_low.head()\n",
    "# len(df_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "SPECIALS = \"\".join([\".\", \",\", \";\", \":\", \"!\", \"?\", '\"', \"'\", \"&\", \"#\", \"$\", \"%\"])\n",
    "\n",
    "def preprocess(text: str, only_kor: bool = True):\n",
    "    \"\"\"한국어 문장을 옵션에 맞게 전처리\"\"\"\n",
    "    # 한국어 모음과 특수문자, 숫자 및 영어 제거\n",
    "    if only_kor:\n",
    "        text = re.sub(r\"[^가-힣\\s]+\", \"\", text)\n",
    "    else:\n",
    "        text = re.sub(rf\"[^가-힣ㄱ-ㅎ0-9{SPECIALS}a-zA-Z\\s]+\", \"\", text)\n",
    "\n",
    "    # 연속 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 좌우 불필요한 공백 제거\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df_filtered['dialect_form'] = df_filtered['dialect_form'].apply(preprocess)\n",
    "df_filtered['standard_form'] = df_filtered['standard_form'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21327,
     "status": "ok",
     "timestamp": 1717558957391,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "3bErn39y6G92",
    "outputId": "9fd2f1ff-2d59-4d7c-cfa6-29d83bacf246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, BertTokenizer, AutoTokenizer\n",
    "# Define custom tokenizer class\n",
    "class CustomSPTokenizer(PreTrainedTokenizerFast):\n",
    "    def __init__(self, tokenizer_path):\n",
    "        super().__init__(tokenizer_file=tokenizer_path)\n",
    "        self._tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._tokenizer.encode(text).tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self._tokenizer.token_to_id(token)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self._tokenizer.id_to_token(index)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return self._tokenizer.decode(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep + [self.sep_token_id]\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "# Initialize the custom tokenizer\n",
    "tokenizer_path = \"bpe_tokenizer.json\"\n",
    "custom_tokenizer = CustomSPTokenizer(tokenizer_path)\n",
    "# kogpt2_tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\n",
    "# custom_tokenizer = CustomSPTokenizer(kogpt2_tokenizer)\n",
    "\n",
    "# Add padding token to tokenizer\n",
    "custom_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update model with new token embeddings \n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "\n",
    "# model.resize_token_embeddings(len(custom_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(16000, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(custom_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1717558959551,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "r2lDQbck9Gk_",
    "outputId": "8fbfb4ac-d845-4cf3-bce9-60a20b173a53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820082</th>\n",
       "      <td></td>\n",
       "      <td>암장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227859</th>\n",
       "      <td></td>\n",
       "      <td>아닐케라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742561</th>\n",
       "      <td></td>\n",
       "      <td>그 여자 대단하다이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819994</th>\n",
       "      <td></td>\n",
       "      <td>지치못허게</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243337</th>\n",
       "      <td>야</td>\n",
       "      <td>라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224452</th>\n",
       "      <td>아니 근데 나 진짜 웃긴 게 내가 그 다문화 그 아니 다문화가 아니라 아 다문화였나...</td>\n",
       "      <td>아니 근데 나 진짜 웃긴 게 내가 그 다문화 그 아니 다문화가 아니라 아 다문화였나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94492</th>\n",
       "      <td>왜냐하면 진짜 이렇게 민원 넣기 쉽 쉽고 이러니깐 특히 특히 우리 한 여기 뭐냐 일...</td>\n",
       "      <td>왜냐하면 진짜 이렇게 민원 넣기 쉽 쉽고 이러니깐 특히 특히 우리 한 여기 뭐냐 일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656625</th>\n",
       "      <td>빗대어서 아우 직설적으로 말하면게게 지 여자친구 싫다고 하는 거니깐 그건 안 되고 ...</td>\n",
       "      <td>빗대어그냉 아우 직설적으로 말하면게게 지 여자친구 싫댄 하는 거난 그건 안 되고 어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94484</th>\n",
       "      <td>칠십팔 그래서 아 내가 딱 봐도 이게 컷이 그때는 문제가 원래 한자 시험이 진짜 개...</td>\n",
       "      <td>칠십팔 그래서 아 내가 딱 봐도 이게 컷이 그때는 문제가 원래 한자 시험이 진짜 개...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94485</th>\n",
       "      <td>이런 식이였어 그래서 원래 문제가 좀 정답률이 위주여야 되는 그런 식으로가 나왔었는...</td>\n",
       "      <td>이런 식이었언 그래서 원래 문제가 좀 정답률이 위주여야 되는 그런 식으로가 나왔었는...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1116389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            standard_form  \\\n",
       "820082                                                      \n",
       "227859                                                      \n",
       "742561                                                      \n",
       "819994                                                      \n",
       "243337                                                  야   \n",
       "...                                                   ...   \n",
       "224452  아니 근데 나 진짜 웃긴 게 내가 그 다문화 그 아니 다문화가 아니라 아 다문화였나...   \n",
       "94492   왜냐하면 진짜 이렇게 민원 넣기 쉽 쉽고 이러니깐 특히 특히 우리 한 여기 뭐냐 일...   \n",
       "656625  빗대어서 아우 직설적으로 말하면게게 지 여자친구 싫다고 하는 거니깐 그건 안 되고 ...   \n",
       "94484   칠십팔 그래서 아 내가 딱 봐도 이게 컷이 그때는 문제가 원래 한자 시험이 진짜 개...   \n",
       "94485   이런 식이였어 그래서 원래 문제가 좀 정답률이 위주여야 되는 그런 식으로가 나왔었는...   \n",
       "\n",
       "                                             dialect_form  \n",
       "820082                                                 암장  \n",
       "227859                                               아닐케라  \n",
       "742561                                         그 여자 대단하다이  \n",
       "819994                                              지치못허게  \n",
       "243337                                                  라  \n",
       "...                                                   ...  \n",
       "224452  아니 근데 나 진짜 웃긴 게 내가 그 다문화 그 아니 다문화가 아니라 아 다문화였나...  \n",
       "94492   왜냐하면 진짜 이렇게 민원 넣기 쉽 쉽고 이러니깐 특히 특히 우리 한 여기 뭐냐 일...  \n",
       "656625  빗대어그냉 아우 직설적으로 말하면게게 지 여자친구 싫댄 하는 거난 그건 안 되고 어...  \n",
       "94484   칠십팔 그래서 아 내가 딱 봐도 이게 컷이 그때는 문제가 원래 한자 시험이 진짜 개...  \n",
       "94485   이런 식이었언 그래서 원래 문제가 좀 정답률이 위주여야 되는 그런 식으로가 나왔었는...  \n",
       "\n",
       "[1116389 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 3/10만 사용함. \n",
    "# df_sampled = df_filtered.sample(frac=0.3, random_state=42)  # Adjust the fraction as needed\n",
    "# # df_sampled.info()\n",
    "# df_sampled\n",
    "\n",
    "# 길이 순 정렬\n",
    "df_sampled = df_filtered\n",
    "df_sampled['standard_form_length'] = df_sampled['standard_form'].apply(lambda x: len(x))\n",
    "df_sampled = df_sampled.sort_values('standard_form_length')\n",
    "df_sampled = df_sampled.drop('standard_form_length', axis=1)\n",
    "# df_sampled.info()\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# spaCy 한국어 모델 로드\n",
    "nlp = spacy.load('ko_core_news_sm')\n",
    "\n",
    "# 데이터 로드 (CSV 파일에서 데이터를 불러온다고 가정)\n",
    "df  = df_sampled #pd.read_csv('your_data.csv', encoding='utf-8')\n",
    "\n",
    "def calculate_similarity(row):\n",
    "    # 표준어와 방언 문장을 spaCy 문서 객체로 변환\n",
    "    doc1 = nlp(row['standard_form'])\n",
    "    doc2 = nlp(row['dialect_form'])\n",
    "    \n",
    "    # 두 문장 간의 유사도 계산\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# 유사도 계산 및 새로운 열 추가\n",
    "df['similarity'] = df.apply(calculate_similarity, axis=1)\n",
    "\n",
    "# 유사도를 기준으로 내림차순 정렬\n",
    "df_sorted = df.sort_values(by='similarity', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "print(df_sorted[['standard_form','dialect_form', 'similarity']])\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "df_sorted.to_csv('jeju_dialect_similarity_results_sorted.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.to_csv('jeju.csv', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sampled = df_filtered.sample(frac=0.3, random_state=42)  # Adjust the fraction as needed\n",
    "# df_sampled['standard_form_length'] = df_sampled['standard_form'].apply(lambda x: len(x))\n",
    "# df_sampled = df_sampled.sort_values('standard_form_length')\n",
    "# df_sampled = df_sampled.drop('standard_form_length', axis=1)\n",
    "# # df_sampled.info()\n",
    "# df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "referenced_widgets": [
      "9c647c6a1907499d8a0b9c353334a7ea",
      "c726985f51fe4d0e896e26e9bc6f0415",
      "38ac8cd65ad0418aa6c93c4a054b8841",
      "72a2f4a1b5f64a2f9d34d51c1b47524f",
      "68a7fc27a4534e0b89eda0b02ec7fee3",
      "5e815f88a0844455bf9bacf8f9041833",
      "5efbe5f28a4b4271b7dd214cfcf4a278",
      "fb51b8518bd442ab83d0a86cf39508b3",
      "fb150f26783640d6ad0e82086e5b89d0",
      "c086d6c6440c43bb924b7dcf0183dcaf",
      "ce2901ccb57642c3baf7bcf3796398bf",
      "6dfaba57c0274e229e32c32a56ed24c6",
      "68a172f79c9a4ec6bbc6eaea72080cb5",
      "589e53b07d6d401b9984f255d6044d0f",
      "f4e2458f25394419b95aa16065465fa9",
      "ad24967179b640b3bd27bb49202e76d4",
      "ef59a58a6cf1499fbc77009804c3e27e",
      "78af7e8e623c402dbbafaeec1c9af795",
      "bc7f30e78a9d4f06a3a48bb83aaaeed0",
      "c294095341974ceb83b5b9ad0e61fd58",
      "31c8cc3780b74d6592880528ada9f8e2",
      "e5eb6704f7364c04b244c5070ced9b1c"
     ]
    },
    "executionInfo": {
     "elapsed": 34514,
     "status": "ok",
     "timestamp": 1717558997149,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "tXcmNuSK4AHi",
    "outputId": "b8f57d9a-d0ce-486d-b27e-afa50fe2f72c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e3ed1e72b04f22829449d7c8113d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/351003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded55a4efb504087b473cbaba3644ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87751 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'labels']\n",
      "['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(df_sampled, test_size=0.2, random_state=42)   \n",
    "\n",
    "# Convert to Hugging Face datasets format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "#########################################################################################################################\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['standard_form']]\n",
    "    targets = [ex for ex in examples['dialect_form']]\n",
    "    # inputs = [ex for ex in examples['dialect_form']]\n",
    "    # targets = [ex for ex in examples['standard_form']]\n",
    "    model_inputs = custom_tokenizer(inputs, max_length=100, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with custom_tokenizer.as_target_tokenizer():\n",
    "        labels = custom_tokenizer(targets, max_length=100, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "#########################################################################################################################\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Filter out unwanted columns\n",
    "train_dataset = train_dataset.remove_columns(['standard_form', 'dialect_form', '__index_level_0__', 'token_type_ids','similarity'])\n",
    "eval_dataset = eval_dataset.remove_columns(['standard_form', 'dialect_form', '__index_level_0__', 'token_type_ids','similarity'])\n",
    "print(train_dataset.column_names)  \n",
    "print(eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3825,
     "status": "ok",
     "timestamp": 1717559006182,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "H6Uaz4sXzBX5",
    "outputId": "4212bb39-d67b-4966-f4ef-ae00785ed9d4"
   },
   "outputs": [],
   "source": [
    "# Load the BART model\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "model.to(device)\n",
    "\n",
    "# 로그 디렉토리 경로 설정\n",
    "log_dir = r\"C:\\Users\\user\\Desktop\\logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",  # 'epoch' 대신 'steps'로 설정하여 주기적으로 평가\n",
    "    eval_steps=500,  # 평가 주기를 500 스텝으로 설정 BART는 5000주기\n",
    "    save_steps=1000,  # 체크포인트 저장 주기를 1000 스텝으로 설정\n",
    "    learning_rate=2e-5,  # 일반적으로 2e-5와 같은 낮은 학습률이 안정적\n",
    "    per_device_train_batch_size=32,  # GPU 메모리와 성능에 따라 조정  Bart = 16. KoBART = 32\n",
    "    per_device_eval_batch_size=32,  # 평가 시에도 배치 크기를 맞춤  Bart = 16. KoBART = 32\n",
    "    num_train_epochs=3,  # 기본 에포크 수\n",
    "    weight_decay=0.01,  # 가중치 감쇠\n",
    "    logging_dir=log_dir,  # 로그 디렉토리\n",
    "    logging_steps=100,  # 로그 주기를 100 스텝으로 설정  Bart = 500. KoBART = 100\n",
    "    save_total_limit=2,  # 저장할 체크포인트 수 제한\n",
    "    load_best_model_at_end=True,  # 학습이 끝난 후 가장 좋은 모델을 로드\n",
    "    gradient_accumulation_steps=2,  # 그래디언트 누적을 통해 배치 크기 증가 효과\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=custom_tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,   #Bart = 16. KoBART = 32\n",
    "    per_device_eval_batch_size=32,    #Bart = 16. KoBART = 32\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=log_dir, \n",
    "    logging_steps=100,                # 로그 주기를 100 스텝으로 설정  Bart = 500. KoBART = 100\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "id": "hYHGpg6P8fRf",
    "outputId": "d5e949e9-97ce-4629-b160-79d3de8fa0eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:587: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16452' max='16452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16452/16452 3:41:48, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.170716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.137010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.120547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.110719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.103277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.100270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.094459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.089762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.086961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.086102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.083016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.081486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.079276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.078019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.076032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.075821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.073268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.072716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.070783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.070129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.069681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.069257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.068454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.068238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.067379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.067037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.067158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.067046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.066293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.066513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.066210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./kobart_highsimilar_standard_to_dialect\")\n",
    "# model.save_pretrained(\"./translated_model_bart100_sort_dialect_to_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프가 끝난 후\n",
    "# 모델 전체 저장 (구조와 가중치 모두 포함)\n",
    "torch.save(model, \"kobart_highsimilar_standard_to_dialect.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:587: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41865' max='41865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41865/41865 6:31:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.050493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.050143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.050001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.049661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.049751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.049291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.049387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.048877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.048644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.048935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.048304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.048116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.048139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.048058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.047910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.047765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.047694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.047350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.047414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.047207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.047341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.047043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.047143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.046918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.046944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.046850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.046802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.046692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.046601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.046544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.046538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.046536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.046490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.046474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=41865, training_loss=0.020505461680850687, metrics={'train_runtime': 23483.7536, 'train_samples_per_second': 114.093, 'train_steps_per_second': 1.783, 'total_flos': 1.59539747447808e+17, 'train_loss': 0.020505461680850687, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작 또는 재개\n",
    "trainer.train(resume_from_checkpoint=r'C:\\Users\\user\\Desktop\\2024-1\\자연어처리\\results\\checkpoint-24000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:587: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83730' max='83730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83730/83730 13:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=83730, training_loss=0.003422350776988546, metrics={'train_runtime': 818.1575, 'train_samples_per_second': 3274.838, 'train_steps_per_second': 102.34, 'total_flos': 1.59539747447808e+17, 'train_loss': 0.003422350776988546, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작 또는 재개\n",
    "trainer.train(resume_from_checkpoint=r'C:\\Users\\user\\Desktop\\2024-1\\자연어처리\\results\\checkpoint-80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:587: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83730' max='83730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83730/83730 13:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=83730, training_loss=0.003422350776988546, metrics={'train_runtime': 818.1575, 'train_samples_per_second': 3274.838, 'train_steps_per_second': 102.34, 'total_flos': 1.59539747447808e+17, 'train_loss': 0.003422350776988546, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작 또는 재개\n",
    "trainer.train(resume_from_checkpoint=r'C:\\Users\\user\\Desktop\\2024-1\\자연어처리\\results\\checkpoint-80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./translated_model_kobart100_sort_dialect_to_standard\")\n",
    "# model.save_pretrained(\"./translated_model_bart100_sort_dialect_to_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\koreanNLP\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'input_ids': tensor([[    2,  9993, 14034, 14034, 14035, 14035, 14035, 14035, 14035, 14034,\n",
      "         14035, 14035, 14034, 14034, 14034, 14034, 14035, 14035, 14034, 14034,\n",
      "         14034, 14034, 14034, 14035, 14035, 14035, 14035, 14035, 14035,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "139\n",
      "Standard: 그렇게 귤이 좋으면 귤만 먹고 살아\n",
      "Dialect: 3571 9417 9417 9418 9418 9418 9418 9418 9417 9418 9418 9417 9417 9417 9417 9418 9418 9417 9417 9417 9417 9417 9418 9418 9418 9418 9418 9418\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드 (모델이 저장된 경로로 수정)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "\n",
    "\n",
    "def translate(text):\n",
    "    # 입력 텍스트를 토크나이징\n",
    "    inputs = custom_tokenizer(text, return_tensors=\"pt\")\n",
    "    # token_type_ids 키 제거\n",
    "    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    \n",
    "    # 예측 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        # max_length=len(inputs['input_ids'][0]),\n",
    "        num_beams=4,\n",
    "        # no_repeat_ngram_size=15,  # 반복 방지 설정\n",
    "        early_stopping=True\n",
    "    )\n",
    "   \n",
    "    # 생성된 토큰을 텍스트로 디코딩\n",
    "    translated_text = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output = custom_tokenizer(translated_text, return_tensors=\"pt\")\n",
    "    print(output)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 예제 번역\n",
    "# sample_sentence = \"잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아.\"\n",
    "# sample_sentence = \"그렇게 공부하면 시험 잘보겠어. 화이팅하자\"\n",
    "sample_sentence = \"그렇게 귤이 좋으면 귤만 먹고 살아\"\n",
    "translated_sentence = translate(sample_sentence)\n",
    "print(len(translated_sentence))\n",
    "print(f\"Standard: {sample_sentence}\")\n",
    "print(f\"Dialect: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'input_ids': tensor([[    2,  2477,   903,  2008,  1288, 14459,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "11\n",
      "Standard: 토라져 버리는거야\n",
      "Dialect: 토 라 져 부 는거라\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드 (모델이 저장된 경로로 수정)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./translated_model_kobart100_sort_standart_to_dialect\")\n",
    "\n",
    "\n",
    "def translate(text):\n",
    "    # 입력 텍스트를 토크나이징\n",
    "    inputs = custom_tokenizer(text, return_tensors=\"pt\")\n",
    "    # token_type_ids 키 제거\n",
    "    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    \n",
    "    # 예측 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        # max_length=len(inputs['input_ids'][0]),\n",
    "        num_beams=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "   \n",
    "    # 생성된 토큰을 텍스트로 디코딩\n",
    "    translated_text = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output = custom_tokenizer(translated_text, return_tensors=\"pt\")\n",
    "    print(output)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 예제 번역\n",
    "# sample_sentence = \"잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아.\"\n",
    "sample_sentence = \"토라져 버리는거야\"\n",
    "# sample_sentence = \"저는 이 사람이 좋습니다\"\n",
    "translated_sentence = translate(sample_sentence)\n",
    "print(len(translated_sentence))\n",
    "print(f\"Standard: {sample_sentence}\")\n",
    "print(f\"Dialect: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "{'input_ids': tensor([[   2, 3261, 2929, 5649, 1191, 2906, 7602, 4262,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "21\n",
      "Standard: 겅 행 맨날 밥 하고 피곤 하겠다\n",
      "Dialect: 그렇게 해서 맨날 밥 하고 피곤 하겠다\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드 (모델이 저장된 경로로 수정)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./translated_model_kobart100_sort_dialect_to_standard\")\n",
    "\n",
    "\n",
    "def translate(text):\n",
    "    # 입력 텍스트를 토크나이징\n",
    "    inputs = custom_tokenizer(text, return_tensors=\"pt\")\n",
    "    # token_type_ids 키 제거\n",
    "    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    \n",
    "    # 예측 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        # max_length=len(inputs['input_ids'][0]),\n",
    "        num_beams=2,\n",
    "        no_repeat_ngram_size=15,  # 반복 방지 설정\n",
    "        early_stopping=True\n",
    "    )\n",
    "   \n",
    "    # 생성된 토큰을 텍스트로 디코딩\n",
    "    translated_text = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output = custom_tokenizer(translated_text, return_tensors=\"pt\")\n",
    "    print(output)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 예제 번역\n",
    "# sample_sentence = \"잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아.\"\n",
    "sample_sentence = \"겅 행 맨날 밥 하고 피곤 하겠다\"\n",
    "translated_sentence = translate(sample_sentence)\n",
    "print(len(translated_sentence))\n",
    "print(f\"Standard: {sample_sentence}\")\n",
    "print(f\"Dialect: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m sample_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# sample_sentence = \"어제 그렇게 행복했어?\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(translated_sentence))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 입력 텍스트를 토크나이징\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_tokenizer\u001b[49m(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# token_type_ids 키 제거\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드 (모델이 저장된 경로로 수정)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./translated_model_gy\")\n",
    "\n",
    "\n",
    "def translate(text):\n",
    "    # 입력 텍스트를 토크나이징\n",
    "    inputs = custom_tokenizer(text, return_tensors=\"pt\")\n",
    "    # token_type_ids 키 제거\n",
    "    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "    print(len(inputs['input_ids'][0]))\n",
    "    \n",
    "    # 예측 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        # max_length=len(inputs['input_ids'][0]),\n",
    "        num_beams=2,\n",
    "        no_repeat_ngram_size=15,  # 반복 방지 설정\n",
    "        early_stopping=True\n",
    "    )\n",
    "   \n",
    "    # 생성된 토큰을 텍스트로 디코딩\n",
    "    translated_text = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output = custom_tokenizer(translated_text, return_tensors=\"pt\")\n",
    "    print(output)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 예제 번역\n",
    "sample_sentence = \"잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아.\"\n",
    "# sample_sentence = \"어제 그렇게 행복했어?\"\n",
    "translated_sentence = translate(sample_sentence)\n",
    "print(len(translated_sentence))\n",
    "print(f\"Standard: {sample_sentence}\")\n",
    "print(f\"Dialect: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1717556009305,
     "user": {
      "displayName": "서가연",
      "userId": "09789171349553603896"
     },
     "user_tz": -540
    },
    "id": "vgsUt7Dp9WXN",
    "outputId": "c135a79b-0d43-402c-f9d2-f9623de1179b"
   },
   "outputs": [],
   "source": [
    "# #model.save_pretrained(\"/content/drive/MyDrive/NLP/translated_model\")\n",
    "# # model = AutoModelForSeq2SeqLM.from_pretrained(\"./translated_model\")\n",
    "\n",
    "\n",
    "# def translate(text):\n",
    "#     inputs = custom_tokenizer(text, return_tensors=\"pt\")\n",
    "#     inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}  # Remove token_type_ids\n",
    "#     print(inputs)\n",
    "#     outputs = model.generate(**inputs, max_length=100, num_beams=4, early_stopping=True)\n",
    "#     translated_text = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return translated_text\n",
    "\n",
    "# # Example translation\n",
    "# sample_sentence = \"예전에 그렇게\"\n",
    "# translated_sentence = translate(sample_sentence)\n",
    "# print(f\"Standard: {sample_sentence}\")\n",
    "# print(f\"Dialect: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/58.0 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/58.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 20.5/58.0 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 41.0/58.0 kB 326.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 58.0/58.0 kB 304.5 kB/s eta 0:00:00\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\anaconda3\\envs\\koreannlp\\lib\\site-packages (from sacrebleu) (2024.4.28)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\koreannlp\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\koreannlp\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\anaconda3\\envs\\koreannlp\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\anaconda3\\envs\\koreannlp\\lib\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "   ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/106.7 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 106.7/106.7 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: tabulate, portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.8.2 sacrebleu-2.4.2 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 점수: 44.26\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    bleu_score = corpus_bleu(hypotheses, [references])\n",
    "    return bleu_score.score\n",
    "\n",
    "# 예시 사용법\n",
    "reference_sentences = [\n",
    "    \"그렇게해서 맨날 밥 하고 피곤하겠다\",\n",
    "    \"잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아\",\n",
    "    \"어제 그렇게 행복했어?\"\n",
    "]\n",
    "\n",
    "translated_sentences = [\n",
    "    \"그렇게해서 맨날 밥 하고 피곤하겠다\",\n",
    "    \"잡아서 온 거 보난 한 대 여섯 마리 잡아서 온 것 닮아.\",\n",
    "    \"어제 겅 행복 핸?\"\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentences, translated_sentences)\n",
    "print(f\"BLEU 점수: {bleu_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111639 entries, 552182 to 964585\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   standard_form  111639 non-null  object\n",
      " 1   dialect_form   111639 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Sample a small portion of the dataset\n",
    "df_sampled = df_filtered.sample(frac=0.1, random_state=42)  # Adjust the fraction as needed\n",
    "df_sampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_form</th>\n",
       "      <th>dialect_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그렇게해서 맨날 밥 하고 그걸로 밥하고 뭐 하고 다 -해-(해나시녜)/(했었어)</td>\n",
       "      <td>경행 맨날 밥 하고 그걸로 밥하고 뭐 하고 다 -해-(해나시녜)/(했었어)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그 쎈 물들을 소는</td>\n",
       "      <td>그 쎈 물들을 쇠는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>각자 메일로 보내자</td>\n",
       "      <td>각자 메일로 보내게</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>기계 아니야?</td>\n",
       "      <td>기계 아이가?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아</td>\n",
       "      <td>잡앙 온 거 보난 혼 대여섯 마리 잡앙 온생이라</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  standard_form  \\\n",
       "0  그렇게해서 맨날 밥 하고 그걸로 밥하고 뭐 하고 다 -해-(해나시녜)/(했었어)   \n",
       "1                                    그 쎈 물들을 소는   \n",
       "2                                    각자 메일로 보내자   \n",
       "3                                       기계 아니야?   \n",
       "4                 잡아서 온 거 보난 한 대여섯 마리 잡아서 온 것같아   \n",
       "\n",
       "                                dialect_form  \n",
       "0  경행 맨날 밥 하고 그걸로 밥하고 뭐 하고 다 -해-(해나시녜)/(했었어)  \n",
       "1                                 그 쎈 물들을 쇠는  \n",
       "2                                 각자 메일로 보내게  \n",
       "3                                    기계 아이가?  \n",
       "4                 잡앙 온 거 보난 혼 대여섯 마리 잡앙 온생이라  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled.reset_index(drop=True, inplace=True)\n",
    "df_sampled.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(df_sampled, test_size=0.2, random_state=42)   ## 89311, 22328\n",
    "# Convert to Hugging Face datasets format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851c0afb72124dc58b686cdfa742bd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721f3f56b50a4133a1a8800947aa9e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained('hyunwoongko/kobart')\n",
    "kobart_model = BartForConditionalGeneration.from_pretrained('hyunwoongko/kobart')\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['standard_form']]\n",
    "    targets = [ex for ex in examples['dialect_form']]\n",
    "    model_inputs = kobart_tokenizer(inputs, max_length=100, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with kobart_tokenizer.as_target_tokenizer():\n",
    "        labels = kobart_tokenizer(targets, max_length=100, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "kobart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f3b697e16540bbaaab27fde173e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3109, 'grad_norm': 1.2133976221084595, 'learning_rate': 4.552131852382659e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2179, 'grad_norm': 0.9054189920425415, 'learning_rate': 4.104263704765317e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1897, 'grad_norm': 0.9314247369766235, 'learning_rate': 3.6563955571479754e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1761, 'grad_norm': 0.7848033905029297, 'learning_rate': 3.208527409530634e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1672, 'grad_norm': 0.7127752900123596, 'learning_rate': 2.7606592619132925e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1572, 'grad_norm': 0.6058608293533325, 'learning_rate': 2.3127911142959513e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1546, 'grad_norm': 0.6911290884017944, 'learning_rate': 1.86492296667861e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1471, 'grad_norm': 0.5462196469306946, 'learning_rate': 1.4170548190612683e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1496, 'grad_norm': 0.6786472797393799, 'learning_rate': 9.69186671443927e-06, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1441, 'grad_norm': 0.8744363784790039, 'learning_rate': 5.213185238265854e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1419, 'grad_norm': 0.6197181940078735, 'learning_rate': 7.3450376209244e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9dffee3e044217ad100a24fd8949e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13399171829223633, 'eval_runtime': 125.7849, 'eval_samples_per_second': 177.509, 'eval_steps_per_second': 11.098, 'epoch': 1.0}\n",
      "{'train_runtime': 2052.9032, 'train_samples_per_second': 43.505, 'train_steps_per_second': 2.719, 'train_loss': 0.177384459728923, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5582, training_loss=0.177384459728923, metrics={'train_runtime': 2052.9032, 'train_samples_per_second': 43.505, 'train_steps_per_second': 2.719, 'total_flos': 5317985627136000.0, 'train_loss': 0.177384459728923, 'epoch': 1.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./kobart_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,  # Adjust learning rate if needed\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "kobart_model.save_pretrained(\"./kobart_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobart_model.to(device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNVLH/WMMsmzWvJ12WD3Svf",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "31c8cc3780b74d6592880528ada9f8e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38ac8cd65ad0418aa6c93c4a054b8841": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb51b8518bd442ab83d0a86cf39508b3",
      "max": 89311,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb150f26783640d6ad0e82086e5b89d0",
      "value": 89311
     }
    },
    "589e53b07d6d401b9984f255d6044d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc7f30e78a9d4f06a3a48bb83aaaeed0",
      "max": 22328,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c294095341974ceb83b5b9ad0e61fd58",
      "value": 22328
     }
    },
    "5e815f88a0844455bf9bacf8f9041833": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5efbe5f28a4b4271b7dd214cfcf4a278": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68a172f79c9a4ec6bbc6eaea72080cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef59a58a6cf1499fbc77009804c3e27e",
      "placeholder": "​",
      "style": "IPY_MODEL_78af7e8e623c402dbbafaeec1c9af795",
      "value": "Map: 100%"
     }
    },
    "68a7fc27a4534e0b89eda0b02ec7fee3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dfaba57c0274e229e32c32a56ed24c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68a172f79c9a4ec6bbc6eaea72080cb5",
       "IPY_MODEL_589e53b07d6d401b9984f255d6044d0f",
       "IPY_MODEL_f4e2458f25394419b95aa16065465fa9"
      ],
      "layout": "IPY_MODEL_ad24967179b640b3bd27bb49202e76d4"
     }
    },
    "72a2f4a1b5f64a2f9d34d51c1b47524f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c086d6c6440c43bb924b7dcf0183dcaf",
      "placeholder": "​",
      "style": "IPY_MODEL_ce2901ccb57642c3baf7bcf3796398bf",
      "value": " 89311/89311 [00:25&lt;00:00, 3855.95 examples/s]"
     }
    },
    "78af7e8e623c402dbbafaeec1c9af795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c647c6a1907499d8a0b9c353334a7ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c726985f51fe4d0e896e26e9bc6f0415",
       "IPY_MODEL_38ac8cd65ad0418aa6c93c4a054b8841",
       "IPY_MODEL_72a2f4a1b5f64a2f9d34d51c1b47524f"
      ],
      "layout": "IPY_MODEL_68a7fc27a4534e0b89eda0b02ec7fee3"
     }
    },
    "ad24967179b640b3bd27bb49202e76d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc7f30e78a9d4f06a3a48bb83aaaeed0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c086d6c6440c43bb924b7dcf0183dcaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c294095341974ceb83b5b9ad0e61fd58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c726985f51fe4d0e896e26e9bc6f0415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e815f88a0844455bf9bacf8f9041833",
      "placeholder": "​",
      "style": "IPY_MODEL_5efbe5f28a4b4271b7dd214cfcf4a278",
      "value": "Map: 100%"
     }
    },
    "ce2901ccb57642c3baf7bcf3796398bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5eb6704f7364c04b244c5070ced9b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef59a58a6cf1499fbc77009804c3e27e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e2458f25394419b95aa16065465fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31c8cc3780b74d6592880528ada9f8e2",
      "placeholder": "​",
      "style": "IPY_MODEL_e5eb6704f7364c04b244c5070ced9b1c",
      "value": " 22328/22328 [00:07&lt;00:00, 1829.41 examples/s]"
     }
    },
    "fb150f26783640d6ad0e82086e5b89d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb51b8518bd442ab83d0a86cf39508b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
